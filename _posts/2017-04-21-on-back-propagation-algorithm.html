---
layout: post
title: 'On back propagation algorithm '
date: '2017-04-21T11:35:00.001-07:00'
author: daryoush
tags: 
modified_time: '2017-07-17T12:54:42.698-07:00'
blogger_id: tag:blogger.com,1999:blog-5623577383635787454.post-4167974784172840930
blogger_orig_url: http://onfp.blogspot.com/2017/04/on-back-propagation-algorithm.html
---

<br /><br />Nice detailed explanation of back propagation in neural networks:<br /><br /><br /><ul><li><a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">A nice post using straight python</a></li><li>Couple of nice post using numpy &nbsp;<a href="http://iamtrask.github.io/2015/07/12/basic-python-network/">basic neural network</a>&nbsp;and&nbsp;<a href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">RNN</a>.</li><li>MIT class&nbsp;<a href="http://courses.csail.mit.edu/6.034f/ai3/netmath.pdf">notes</a></li><li><a href="http://colah.github.io/posts/2015-08-Backprop/">Graph view</a> of the gradient. &nbsp;The "Computational Victories" has good explanation of why back propagation is the preferred method of gradient calculation.&nbsp;</li></ul><br /><br /><br /><br />