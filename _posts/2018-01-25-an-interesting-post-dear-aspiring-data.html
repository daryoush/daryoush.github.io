---
layout: post
title: Why you cant just skip Deep Learning.
date: '2018-01-25T16:32:00.000-08:00'
author: daryoush
tags: 
modified_time: '2018-01-25T16:52:09.408-08:00'
blogger_id: tag:blogger.com,1999:blog-5623577383635787454.post-4035577044340347578
blogger_orig_url: http://onfp.blogspot.com/2018/01/an-interesting-post-dear-aspiring-data.html
---

An interesting post:&nbsp;&nbsp;<a href="https://thisismetis.com/blog/dear-aspiring-data-scientists-just-skip-deep-learning-for-now?utm_source=mailchimp&amp;utm_medium=email&amp;utm_campaign=012517_blogroundup&amp;utm_source=Metis+Newsletter&amp;utm_campaign=1a00cda5f4-EMAIL_CAMPAIGN_2017_11_14&amp;utm_medium=email&amp;utm_term=0_7f74f3175d-1a00cda5f4-227646525&amp;mc_cid=1a00cda5f4&amp;mc_eid=0492ddfbee">Dear Aspiring Data Scientists, Just Skip Deep Learning (For Now)</a>.<br /><br />Hardly a day goes by that I don't hear the same sentiment expressed in one way or another.&nbsp; &nbsp;The problem is that they are correct and wrong at the same time.<br /><br />The problem is terminology.&nbsp; &nbsp;Let's use the terminology defined in&nbsp;<a href="http://varianceexplained.org/r/ds-ml-ai/">What's the difference between data science, machine learning, and artificial intelligence?</a><br /><br />So far as you are looking at Data Science and Machine Learning (and your focus is to be hired)&nbsp; that is insight and prediction, then the article is valid.&nbsp; &nbsp; &nbsp; If your goal is a prediction, then why not use the simplest method, it is easier to train and generalize.&nbsp; &nbsp;Furthermore, they are correct in that training for image recognition, voice processing, computer vision... you need a massive amount of data and processing power.&nbsp; &nbsp; &nbsp;This is not where most DS/ML jobs are.<br /><br />The problem they are missing, again going back to the definition above, is prediction vs action.&nbsp; &nbsp; &nbsp;So long as the goal of DS/ML is to gather insight and prediction for humans then arguments are valid.&nbsp; &nbsp;It is when you want to take an action that it all falls apart.&nbsp; &nbsp;Humans can potentially apply their domain knowledge and navigate the predictions to find the optimum action.<br /><br />But for machines, it is very different.&nbsp; &nbsp;You basically have three choices to determine the best action, apply human-derived rules to the predictions (1980s AI),&nbsp; &nbsp;reduce the problem to an optimization program (<a href="https://www.math.ucla.edu/~tom/LP.pdf">linear</a> or <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex</a>), or essentially use <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">reinforcement learning</a> to derive a policy to deal with the uncertainty of your prediction.&nbsp; &nbsp; This is I think the essence of <a href="https://medium.com/@karpathy/software-2-0-a64152b37c35">Software 2.0</a>&nbsp;or what I like to call Training Driven Development (TrDD) -- More on this later.<br /><br />If rules and/or optimization works then great you are done.&nbsp; &nbsp;But when that is not an option, then in the model prescribed by the article, you need to combine a policy neural network with your ML prediction.&nbsp; &nbsp; &nbsp; The problem now is that you have two islands to deal with, your object loss function's gradient from neural network can't propagate to your ML prediction.&nbsp; &nbsp; Simplifications that worked so well for ML prediction for humans now are being amplified as errors in your policy network.&nbsp; &nbsp; I don't know how you can have a loss function that can train the policy and communicate with the say a linear regression model's loss function.<br /><br />After reading&nbsp;<a href="http://chris-said.io/2016/05/11/optimizing-things-in-the-ussr/">Optimizing things in the USSR</a>&nbsp;I ordered my&nbsp;<a href="https://www.amazon.com/Red-Plenty-Francis-Spufford/dp/1555976042">Red Plenty</a>&nbsp;book.&nbsp; It has some interesting observation as to what happens&nbsp;when you "simplify assumptions".